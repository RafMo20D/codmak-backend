{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ed61d1-00f1-4a54-8f2e-e99ca058a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c50b6d6-2739-4f1c-887c-9ce8b5489c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.31.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b18098-e78f-4bba-92f8-e00ce63f0c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data_for_bert.csv' is your cleaned dataset with 'BERT_Input' and 'Transport_Mode' columns\n",
    "data_path = 'traffic-report-cleaned-for-bert.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize all texts and map the tokens to thier word IDs\n",
    "input_ids = []\n",
    "attention_masks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3281f720-6ede-42ee-a367-47089247f8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64]) torch.Size([2, 64]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Example DataFrame column 'BERT_Input'\n",
    "df = pd.DataFrame({\n",
    "    'BERT_Input': [\n",
    "        \"2023-03-01 to 2023-03-31. Al Baha to Makhwa Road\",\n",
    "        \"2023-03-01 to 2023-03-31. Baljuraishi to Al Baha Road - Transfered\"\n",
    "    ],\n",
    "    'Transport_Mode_Labels': [0, 1]  # Example labels, replace with your actual labels\n",
    "})\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Initialize lists to store the encoded information\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# Encoding each sentence in the dataset\n",
    "for text in df['BERT_Input']:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,                      # Sentence to encode.\n",
    "        add_special_tokens=True,   # Add '[CLS]' and '[SEP]'\n",
    "        max_length=64,             # Pad or truncate.\n",
    "        padding='max_length',      # Pad to max length\n",
    "        return_attention_mask=True,# Construct attn. masks.\n",
    "        return_tensors='pt',       # Return pytorch tensors.\n",
    "    )\n",
    "    \n",
    "    # Add the encoded sentence to the list. \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(df['Transport_Mode_Labels'].values)\n",
    "\n",
    "print(input_ids.shape, attention_masks.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a643f12-c3fd-4d1e-a489-8d2b159a2069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3014d65-47b7-4537-8a1b-4a318894bbd4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=42, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=42, test_size=0.1)\n",
    "\n",
    "# Convert to DataLoader\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47c224e9-3af9-4ea7-89c3-e576008f0032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 11:50:18.976419: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dc8a4ec-e36e-4b8e-9655-9d4c00630ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=df['Transport_Mode_Labels'].nunique())\n",
    "\n",
    "# Specify the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer & Learning Rates\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
    "epochs = 4\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8e5884f-5403-4795-b0cf-950e066b5030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.7634192109107971\n",
      "Average training loss: 0.5685696005821228\n",
      "Average training loss: 0.5655654668807983\n",
      "Average training loss: 0.3588297367095947\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Average training loss: {avg_train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20e3fc4a-bb8c-430b-8f35-714816b39990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.3588297367095947\n",
      "Validation Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average training loss: {avg_train_loss}\")\n",
    "\n",
    "model.eval()\n",
    "eval_accuracy, eval_steps = 0, 0\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "for batch in validation_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    with torch.no_grad():        \n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tmp_eval_accuracy = accuracy_score(label_ids, np.argmax(logits, axis=1))\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += b_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "print(f\"Validation Accuracy: {eval_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39d82745-1b4b-4795-81af-3d04da763954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0\n",
      "Validation Precision: 0.0\n",
      "Validation Recall: 0.0\n",
      "Validation F1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Assuming you've already defined compute_metrics or you're directly using sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# Add the missing part of the validation loop here for clarity\n",
    "# Ensure logits and label_ids are correctly handled\n",
    "\n",
    "# Assuming the loop over validation_dataloader is correctly set\n",
    "for batch in validation_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    logits = logits.detach().cpu().numpy()  # Ensure logits are numpy arrays\n",
    "    label_ids = b_labels.to('cpu').numpy()  # Ensure label_ids are numpy arrays\n",
    "\n",
    "    # Now calculate metrics\n",
    "    pred_labels = np.argmax(logits, axis=1)\n",
    "    accuracy = accuracy_score(label_ids, pred_labels)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(label_ids, pred_labels, average='weighted')\n",
    "\n",
    "    print(f\"Validation Accuracy: {accuracy}\")\n",
    "    print(f\"Validation Precision: {precision}\")\n",
    "    print(f\"Validation Recall: {recall}\")\n",
    "    print(f\"Validation F1: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffceb3c9-0c59-4a5e-a01f-543e7b541e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'fine_tuned_bert_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce1eb427-3dbe-455c-b717-1b9feb3ea3f5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model is fine-tuned now\n"
     ]
    }
   ],
   "source": [
    "print(\"the model is fine-tuned now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a7b0b5-1ded-4e62-8062-c64f20fa099b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
