{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e074728-86c3-41d4-89b3-930c5c8ca1c1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22dbe2ee-0de8-4783-b7b0-e9f8d5091280",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"traffic-report-mar-2023.csv\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7463f9f4-264c-41c4-a813-f9921f71d497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Node</th>\n",
       "      <th>Site ID</th>\n",
       "      <th>Site Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>Av Speed</th>\n",
       "      <th>85th %ile</th>\n",
       "      <th>24 Hour</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(km/h)</td>\n",
       "      <td>(km/h)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>SAUDI</td>\n",
       "      <td>10012460.0</td>\n",
       "      <td>10012460.0</td>\n",
       "      <td>Al Baha to Makhwa Road</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>SAUDI</td>\n",
       "      <td>10020150.0</td>\n",
       "      <td>10020150.0</td>\n",
       "      <td>Baljuraishi to Al Baha Road - Transfered</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>SAUDI</td>\n",
       "      <td>10020151.0</td>\n",
       "      <td>10020151.0</td>\n",
       "      <td>Al Baha to Baljuraishi Road - Transferred</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>SAUDI</td>\n",
       "      <td>10023400.0</td>\n",
       "      <td>10023400.0</td>\n",
       "      <td>Al Baha to Al Aqiq Road - Road No. 3400 - Tran...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>SAUDI</td>\n",
       "      <td>80700151.0</td>\n",
       "      <td>80700151.0</td>\n",
       "      <td>Najran to Khamis Mushyit Road</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>SAUDI</td>\n",
       "      <td>80802090.0</td>\n",
       "      <td>80802090.0</td>\n",
       "      <td>Sheaar to Amohaiyl Road</td>\n",
       "      <td>86.6</td>\n",
       "      <td>108</td>\n",
       "      <td>5408</td>\n",
       "      <td>167651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>SAUDI</td>\n",
       "      <td>80902500.0</td>\n",
       "      <td>80902500.0</td>\n",
       "      <td>Bisha to Al Alaia Road</td>\n",
       "      <td>93.9</td>\n",
       "      <td>111</td>\n",
       "      <td>1893</td>\n",
       "      <td>58670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>SAUDI</td>\n",
       "      <td>90316910.0</td>\n",
       "      <td>90316910.0</td>\n",
       "      <td>Kodmi to Harud Road</td>\n",
       "      <td>84.5</td>\n",
       "      <td>103</td>\n",
       "      <td>2717</td>\n",
       "      <td>84217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>2023-03-01</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>SAUDI</td>\n",
       "      <td>90415700.0</td>\n",
       "      <td>90415700.0</td>\n",
       "      <td>Abu Arish to Ahad Al Masarihan</td>\n",
       "      <td>93.1</td>\n",
       "      <td>110</td>\n",
       "      <td>16681</td>\n",
       "      <td>517038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           From          To   Node     Site ID   Site Name  \\\n",
       "0           NaN         NaN    NaN         NaN         NaN   \n",
       "1    2023-03-01  2023-03-31  SAUDI  10012460.0  10012460.0   \n",
       "2    2023-03-01  2023-03-31  SAUDI  10020150.0  10020150.0   \n",
       "3    2023-03-01  2023-03-31  SAUDI  10020151.0  10020151.0   \n",
       "4    2023-03-01  2023-03-31  SAUDI  10023400.0  10023400.0   \n",
       "..          ...         ...    ...         ...         ...   \n",
       "244  2023-03-01  2023-03-31  SAUDI  80700151.0  80700151.0   \n",
       "245  2023-03-01  2023-03-31  SAUDI  80802090.0  80802090.0   \n",
       "246  2023-03-01  2023-03-31  SAUDI  80902500.0  80902500.0   \n",
       "247  2023-03-01  2023-03-31  SAUDI  90316910.0  90316910.0   \n",
       "248  2023-03-01  2023-03-31  SAUDI  90415700.0  90415700.0   \n",
       "\n",
       "                                           Description Av Speed 85th %ile  \\\n",
       "0                                                  NaN   (km/h)    (km/h)   \n",
       "1                               Al Baha to Makhwa Road        -         -   \n",
       "2             Baljuraishi to Al Baha Road - Transfered        -         -   \n",
       "3            Al Baha to Baljuraishi Road - Transferred        -         -   \n",
       "4    Al Baha to Al Aqiq Road - Road No. 3400 - Tran...        -         -   \n",
       "..                                                 ...      ...       ...   \n",
       "244                      Najran to Khamis Mushyit Road        -         -   \n",
       "245                            Sheaar to Amohaiyl Road     86.6       108   \n",
       "246                             Bisha to Al Alaia Road     93.9       111   \n",
       "247                                Kodmi to Harud Road     84.5       103   \n",
       "248                     Abu Arish to Ahad Al Masarihan     93.1       110   \n",
       "\n",
       "    24 Hour   Total  \n",
       "0       NaN     NaN  \n",
       "1         -       -  \n",
       "2         -       -  \n",
       "3         -       -  \n",
       "4         -       -  \n",
       "..      ...     ...  \n",
       "244       -       -  \n",
       "245    5408  167651  \n",
       "246    1893   58670  \n",
       "247    2717   84217  \n",
       "248   16681  517038  \n",
       "\n",
       "[249 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "113ab70c-4a18-47a2-8516-05581770aba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "         From          To   Node     Site ID   Site Name  \\\n",
      "0         NaN         NaN    NaN         NaN         NaN   \n",
      "1  2023-03-01  2023-03-31  SAUDI  10012460.0  10012460.0   \n",
      "2  2023-03-01  2023-03-31  SAUDI  10020150.0  10020150.0   \n",
      "3  2023-03-01  2023-03-31  SAUDI  10020151.0  10020151.0   \n",
      "4  2023-03-01  2023-03-31  SAUDI  10023400.0  10023400.0   \n",
      "\n",
      "                                         Description Av Speed 85th %ile  \\\n",
      "0                                                NaN   (km/h)    (km/h)   \n",
      "1                             Al Baha to Makhwa Road        -         -   \n",
      "2           Baljuraishi to Al Baha Road - Transfered        -         -   \n",
      "3          Al Baha to Baljuraishi Road - Transferred        -         -   \n",
      "4  Al Baha to Al Aqiq Road - Road No. 3400 - Tran...        -         -   \n",
      "\n",
      "  24 Hour Total  \n",
      "0     NaN   NaN  \n",
      "1       -     -  \n",
      "2       -     -  \n",
      "3       -     -  \n",
      "4       -     -  \n"
     ]
    }
   ],
   "source": [
    "print(\"Original DataFrame:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "241b1fc6-7e7c-4387-baf8-369addd2a35b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.31.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Define the model name\n",
    "model_name = 'bert-base-uncased'  # You can choose different sizes, e.g., 'bert-large-uncased'\n",
    "\n",
    "# Load pre-trained tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Optionally, modify the model for your specific task\n",
    "# For example, if your task involves recommending tourist attractions, you might add a classification layer on top of BERT:\n",
    "# model.classifier = nn.Linear(in_features=model.config.hidden_size, out_features=num_classes)\n",
    "\n",
    "# Print model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "077c72b8-f323-43b9-8333-db684f39f6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          input_text  \\\n",
      "0                                                NaN   \n",
      "1       2023-03-01 2023-03-31 Al Baha to Makhwa Road   \n",
      "2  2023-03-01 2023-03-31 Baljuraishi to Al Baha R...   \n",
      "3  2023-03-01 2023-03-31 Al Baha to Baljuraishi R...   \n",
      "4  2023-03-01 2023-03-31 Al Baha to Al Aqiq Road ...   \n",
      "\n",
      "                               input_text_with_speed  \n",
      "0                                                NaN  \n",
      "1  2023-03-01 2023-03-31 Al Baha to Makhwa Road A...  \n",
      "2  2023-03-01 2023-03-31 Baljuraishi to Al Baha R...  \n",
      "3  2023-03-01 2023-03-31 Al Baha to Baljuraishi R...  \n",
      "4  2023-03-01 2023-03-31 Al Baha to Al Aqiq Road ...  \n",
      "  Av Speed 85th %ile 24 Hour Total\n",
      "0   (km/h)    (km/h)     NaN   NaN\n",
      "1        -         -       -     -\n",
      "2        -         -       -     -\n",
      "3        -         -       -     -\n",
      "4        -         -       -     -\n"
     ]
    }
   ],
   "source": [
    "# Concatenate relevant columns to form input text sequences\n",
    "data['input_text'] = data['From'] + ' ' + data['To'] + ' ' + data['Description']\n",
    "\n",
    "# Example of incorporating numerical features\n",
    "# For demonstration purposes, let's concatenate 'Av Speed' with the input text sequences\n",
    "data['input_text_with_speed'] = data['input_text'] + ' Avg Speed: ' + data['Av Speed'].astype(str)\n",
    "\n",
    "# Prepare numerical features separately\n",
    "numerical_features = data[['Av Speed', '85th %ile', '24 Hour', 'Total']]\n",
    "\n",
    "# Display the first few rows of prepared input data and numerical features\n",
    "print(data[['input_text', 'input_text_with_speed']].head())\n",
    "print(numerical_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d7f6024e-a8d6-4349-bd63-70a295ef8a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in the dataset: 249\n"
     ]
    }
   ],
   "source": [
    "# Check the total number of rows in the DataFrame\n",
    "print(\"Total number of rows in the dataset:\", len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1d735cc-6614-4fac-a725-b3d665a110b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          input_text  \\\n",
      "0                                                NaN   \n",
      "1       2023-03-01 2023-03-31 Al Baha to Makhwa Road   \n",
      "2  2023-03-01 2023-03-31 Baljuraishi to Al Baha R...   \n",
      "3  2023-03-01 2023-03-31 Al Baha to Baljuraishi R...   \n",
      "4  2023-03-01 2023-03-31 Al Baha to Al Aqiq Road ...   \n",
      "\n",
      "                               input_text_with_speed  \n",
      "0                                                NaN  \n",
      "1  2023-03-01 2023-03-31 Al Baha to Makhwa Road A...  \n",
      "2  2023-03-01 2023-03-31 Baljuraishi to Al Baha R...  \n",
      "3  2023-03-01 2023-03-31 Al Baha to Baljuraishi R...  \n",
      "4  2023-03-01 2023-03-31 Al Baha to Al Aqiq Road ...  \n"
     ]
    }
   ],
   "source": [
    "# Print the first few rows of the DataFrame to check if the columns have been created for all rows\n",
    "print(data[['input_text', 'input_text_with_speed']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6ab876-a4a2-4a00-8b19-ad0431aa1e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I want to test the model general text of Bert model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b65bad5-4af3-43c2-b063-f2e4eae66584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce27cf96-bc39-4a16-ae91-0c3834188b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the BERT-based model architecture\n",
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self, bert_model, num_classes):\n",
    "        super(CustomBERTModel, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.fc(outputs.last_hidden_state[:, 0, :])  # Using the [CLS] token representation\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a7c819d-f074-4ae1-8954-956b6c6f3593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for text generation task\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, numerical_features, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.numerical_features = numerical_features\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        numerical_feat = self.numerical_features[idx]\n",
    "\n",
    "        inputs = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        input_ids = inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "        numerical_feat = torch.tensor(numerical_feat, dtype=torch.float32)\n",
    "\n",
    "        return input_ids, attention_mask, numerical_feat, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2808b05d-c896-4a64-a6c5-2a053453f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "num_classes = 2  # Example: Assuming binary classification for the best route and channel\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "learning_rate = 2e-5\n",
    "max_length = 128  # Maximum length of input sequences\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b7570b5-f563-4f0f-8d67-0445288ded2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomBERTModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "config.output_hidden_states = True  # This is required for Qlora\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased', config=config)\n",
    "model = CustomBERTModel(bert_model, num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "553e4131-8ac1-4331-8927-839aa976c130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "29fcecc7-5010-46fd-b3e7-de647e98357b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YourModel()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define your model architecture and import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "# Define your model architecture\n",
    "class YourModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(YourModel, self).__init__()\n",
    "        # Define your model layers here\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, numerical_features):\n",
    "        # Define the forward pass of your model here\n",
    "        return output_logits\n",
    "\n",
    "# Define your model hyperparameters\n",
    "input_size = 768  # Update this with the appropriate input size based on your model's architecture\n",
    "output_size = num_classes  # Update this with the number of output classes in your classification task\n",
    "\n",
    "# Initialize your model\n",
    "model = YourModel(input_size, output_size)\n",
    "\n",
    "# Define other hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Define your device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move your model to the appropriate device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e0bc841-c5d5-4138-9502-06bb0a8d28c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CustomDataset.__init__() takes 6 positional arguments but 7 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m train_texts, val_texts, train_numerical_features, val_numerical_features, train_labels_channel, val_labels_channel, train_labels_mode, val_labels_mode \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     23\u001b[0m     train_texts, train_numerical_features, train_labels_channel, train_labels_mode, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Prepare train, validation, and test datasets\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels_channel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_numerical_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(val_texts, val_labels_channel, val_labels_mode, val_numerical_features, tokenizer, max_length)\n\u001b[1;32m     28\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(test_texts, test_labels_channel, test_labels_mode, test_numerical_features, tokenizer, max_length)\n",
      "\u001b[0;31mTypeError\u001b[0m: CustomDataset.__init__() takes 6 positional arguments but 7 were given"
     ]
    }
   ],
   "source": [
    "transport_channels = ['Road', 'Rail', 'Air', 'Sea']\n",
    "transport_modes = ['Car', 'Bus', 'Train', 'Airplane', 'Ship']\n",
    "\n",
    "# Create label columns for transport channel and mode\n",
    "data['Transport_Channel'] = ''  # Initialize empty column for transport channel\n",
    "data['Transport_Mode'] = ''  # Initialize empty column for transport mode\n",
    "\n",
    "# Label Assignment (Assuming you have additional data or domain knowledge to infer this information)\n",
    "# For demonstration purposes, let's assume that the transport channel is always 'Road' and the mode of transport is 'Car'\n",
    "data['Transport_Channel'] = 'Road'\n",
    "data['Transport_Mode'] = 'Car'\n",
    "\n",
    "# Split data into input texts, numerical features, and labels\n",
    "input_texts = data['input_text'].tolist()\n",
    "numerical_features = data[['Av Speed', '85th %ile', '24 Hour', 'Total']]\n",
    "labels_transport_channel = data['Transport_Channel']\n",
    "labels_transport_mode = data['Transport_Mode']\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train_texts, test_texts, train_numerical_features, test_numerical_features, train_labels_channel, test_labels_channel, train_labels_mode, test_labels_mode = train_test_split(\n",
    "    input_texts, numerical_features, labels_transport_channel, labels_transport_mode, test_size=0.2, random_state=42)\n",
    "train_texts, val_texts, train_numerical_features, val_numerical_features, train_labels_channel, val_labels_channel, train_labels_mode, val_labels_mode = train_test_split(\n",
    "    train_texts, train_numerical_features, train_labels_channel, train_labels_mode, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare train, validation, and test datasets\n",
    "train_dataset = CustomDataset(train_texts, train_labels_channel, train_labels_mode, train_numerical_features, tokenizer, max_length)\n",
    "val_dataset = CustomDataset(val_texts, val_labels_channel, val_labels_mode, val_numerical_features, tokenizer, max_length)\n",
    "test_dataset = CustomDataset(test_texts, test_labels_channel, test_labels_mode, test_numerical_features, tokenizer, max_length)\n",
    "\n",
    "# Define data loaders for train, validation, and test sets\n",
    "batch_size = 32  # Update batch size as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e24c86f1-66fc-4859-87c9-d479b5eb0804",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(train_texts, train_labels_channel, train_labels_mode, tokenizer, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m      3\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(val_texts, val_labels_channel, val_labels_mode, tokenizer, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(\u001b[43mtest_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_text\u001b[39m\u001b[38;5;124m'\u001b[39m], test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransport_Channel\u001b[39m\u001b[38;5;124m'\u001b[39m], test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransport_Mode\u001b[39m\u001b[38;5;124m'\u001b[39m], tokenizer, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m      6\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create data loaders for train, validation, and test sets\n",
    "train_dataset = CustomDataset(train_texts, train_labels_channel, train_labels_mode, tokenizer, max_length=128)\n",
    "val_dataset = CustomDataset(val_texts, val_labels_channel, val_labels_mode, tokenizer, max_length=128)\n",
    "test_dataset = CustomDataset(test_df['input_text'], test_df['Transport_Channel'], test_df['Transport_Mode'], tokenizer, max_length=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for input_ids, attention_mask, labels_channel, labels_mode in train_loader:\n",
    "        input_ids, attention_mask, labels_channel, labels_mode = input_ids.to(device), attention_mask.to(device), labels_channel.to(device), labels_mode.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels_channel)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_preds_channel, val_preds_mode = [], []\n",
    "    val_labels_channel, val_labels_mode = [], []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels_channel, labels_mode in val_loader:\n",
    "            input_ids, attention_mask, labels_channel, labels_mode = input_ids.to(device), attention_mask.to(device), labels_channel.to(device), labels_mode.to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            val_loss = criterion(outputs, labels_channel)\n",
    "            val_losses.append(val_loss.item())\n",
    "            val_preds_channel.extend(torch.argmax(outputs, axis=1).cpu().numpy())\n",
    "            val_preds_mode.extend(torch.argmax(outputs, axis=1).cpu().numpy())\n",
    "            val_labels_channel.extend(labels_channel.cpu().numpy())\n",
    "            val_labels_mode.extend(labels_mode.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    val_accuracy_channel = accuracy_score(val_labels_channel, val_preds_channel)\n",
    "    val_accuracy_mode = accuracy_score(val_labels_mode, val_preds_mode)\n",
    "    val_precision_channel = precision_score(val_labels_channel, val_preds_channel, average='macro')\n",
    "    val_precision_mode = precision_score(val_labels_mode, val_preds_mode, average='macro')\n",
    "    val_recall_channel = recall_score(val_labels_channel, val_preds_channel, average='macro')\n",
    "    val_recall_mode = recall_score(val_labels_mode, val_preds_mode, average='macro')\n",
    "    val_f1_channel = f1_score(val_labels_channel, val_preds_channel, average='macro')\n",
    "    val_f1_mode = f1_score(val_labels_mode, val_preds_mode, average='macro')\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
    "          f'Train Loss: {total_loss/len(train_loader):.4f}, '\n",
    "          f'Val Loss: {np.mean(val_losses):.4f}, '\n",
    "          f'Val Acc (Channel): {val_accuracy_channel:.4f}, '\n",
    "          f'Val Acc (Mode): {val_accuracy_mode:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca437763-cb9d-4e81-bdad-3e51d05a541a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
